# 第二章 图形渲染管线

标签（空格分隔）： 翻译 RTR3

---
    链条的坚固程度取决于它最薄弱的一环。
                                   ——佚名
    
　　本章将向大家展示什么是实时渲染的核心部分,它被称为*图形渲染管线*(graphics rendering pipeline),有时候也简单称作*管线*(pipeline)。管线的主要功能就是在给定了虚拟相机,三维模型,光源,Shader(shading equations),纹理等的情况下去生成或渲染出一张二维图像。渲染管线是实时渲染的基础工具。管线的流程如图2.1所描述。物体在图像上的的位置和形状由他们的几何体,环境特性,以及相机在环境中的位置所决定。物体的外观表现则受到材质属性,光源,纹理以及Shader(shading models)的影响。
　　接下来开始讨论和解释渲染管线的不同阶段,我们把重点放在功能而不是实现上。关于实现的细节要么会在后续章节中解释,要么就是这些部分超出了程序员的控制范围。举个例子,对于使用线条来说,像顶点数据格式,颜色,以及布局格式(pattern types),和景深(depth cueing)是否可用才是我们关心的。而并不是去关注线条绘制是不是基于布氏线条绘制算法(Bresenham's line-drawing algorithm)[142]或基于对称双步算法(symmetric double-step algorithm)。一般这些管线中的一部分都是直接实现在硬件上的,这也就决定了没有办法通过实现来进行优化或提升。关于基础绘制填充算法的详细内容在一些像Rogers的书中都有深入涉及[1077].不过我们也会对基础硬件有一些控制能力,算法和编码方式都会对高速,高质量的生成图像产生有显著的提升。

![Figure2.1](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.1.jpg)
Figure 2.1 在左边的图中,一个虚拟相机位于椎体的尖点上(由四条线交汇的地方)。只有在视见体内部的图元才会被渲染。对于一个要以透视投影渲染的画面(正如这个例子),视见体就是一个截头椎体,就是一个以长方形为底并被切去顶端的椎体。右侧的图展示了相机"看到"了什么。注意左图中那个红色圆环没有出现在右侧画面里是因为它是完全位于视锥体外面的。同样,左图中的哪个蓝色的扭曲棱柱被视锥的上平面给裁剪。

##2.1 架构

　　在物理世界里,管线这个概念有很多不同的意思,从工厂的装配线到滑雪场的上山吊椅。它也被同样的用在图形渲染上。
　　一个管线包含一系列阶段。举例子, 在一个使用管线里,石油不能从第一段管道输送到第二段管道,直到第二段管道的石油已经被输送到了第三段管道,第四段以此类推。这也就意味着管线的速度取决于速度最慢的一段管道,而不论其它段的管道有多快。
　　理想情况下,一个非流水线的系统被分成n段流水线形式可以得到n倍的速度提升。这种性能上的提高也是使用流水线的主要原因。举个例子,滑雪场的上山吊椅只包含一个座位是非常低效的。通过添加更多的座位可以成比例的提高把滑雪者送到山上的速度。流水线式并行执行的。但是他们会出现停滞直到最慢的一段结束它的任务。举个例子,如果在一个汽车生产流水线上方向盘安装这个阶段需要花费3分钟,而其它的阶段都只需要2分钟。这个流水线能达到的最佳速率也就是3分钟生产一辆汽车。其它阶段就必须闲置下来1分钟,等待方向盘安装阶段的结束。对于这个特殊的管线来说。方向盘安装阶段就是一个瓶颈。因为它决定了整个生产流程的速度。
　　这种类型的管线结构也同样能在实时计算机图形学中找到。可以把实时渲染管线可以粗略分为三个概念阶段——应用阶段,几何阶段,以及光栅化阶段——如图2.2所示。
　　![Figure2.2](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.2.png)
　　Figure 2.2 渲染管线的基本结构由三个阶段构成:应用,几何,光栅化。每个阶段自身也可以是一个管线,如图中所展示在几何阶段的下面。或者有的阶段会(部分的)进行并行化处理,像光栅化阶段的下面所展示那样。在这个图中,应用阶段只有一个过程,但是这个阶段也可以同样管道化或者并行化。
　　
这种结构也是一个实时图形应用程序的图形渲染管线引擎的核心部分,也因为它是如此基础,我们会在接下来章节进行讨论。这些阶段自己往往也是一个管线,也就意味着它包含着几个子阶段。我们要把概念阶段(应用阶段,几何阶段,光栅化阶段)和功能阶段,以及管线阶段区分开。一个功能阶段有一个明确的任务要处理,但是并没有规定在管线中的具体执行方式。一个管线阶段,会同时地与其它所有管线执行。一个管线阶段也会被并行化以达到高性能需求。举个例子,几何阶段会被分为5个功能阶段,不过具体被分成几个管线阶段是由图形系统的实现来决定的。一个给定的实现可能把两个功能阶段合并到一个管线阶段,同时它把一个耗时的功能阶段分成多个管线阶段,甚至把它们并行化。
　　渲染速度,也就是画面的更新频率是由管线中最慢阶段决定的。这个速度会以帧每秒的(fps)来表示,也就是每秒钟画面更新多少张图像。它也同样被用赫兹来表示(Hz),是1/seconds的简化符号,表示更新的频率。一个应用程序生成一个图像的耗时往往不同,这依赖于他们每帧之间的计算复杂程度。帧每秒要么表示特定帧的速率,要么表示一段时间内的平均表现。赫兹被用在硬件上。比如显示器,一般被设置为固定帧率。因为我们使用管线来处理,所以不能把我们想要渲染的所有数据数据通过整个管线话费的时间的综合加起来。当然这是由管线结构所导致的。它允许各个阶段并行执行。如果我们定位到了瓶颈,也就是管线中最慢的阶段,并且测量一下数据经过这个阶段需要花费的时间。我们就能计算出渲染的速率了。假设,那个瓶颈阶段花费了20ms(milliseconds)执行相关运算。那么渲染速率就是1/0.020 = 50 Hz。然而这只有当输出设备可以以这个特定速率进行更新的时候才成立。否则真是的输出速率会更低。在一些其它关于流水线的内容里,术语吞吐量(throughput)被用来代替渲染速度。
**例子:渲染速率。**假设我们的输出设备最大更新频率是60Hz.并且渲染管线中的瓶颈阶段也已经被发现。测试结果表示这个阶段花费了62.5ms执行时间。那么渲染速率将按照如下方式进行计算。首先,不考虑输出设备，我们得到最大的渲染速率是1/0.0625=16fps。然后调整这个输出到输出设备的频率：60Hz意味着渲染速率可以是60Hz,60/2=30Hz,60/3=20Hz,60/4=15Hz,60/5=12Hz等等。这表示我们可以期望渲染速率是15Hz,因为这是输出设备可以提供的小于16fps的最大恒定输出速度。
　　正如名字中所隐含的意思,***应用阶段***是由一用程序来驱动的,因此也是由运行在通用CPU上的软件来执行的。这些CPU一般是可以并行处理多线程的多核CPU。这样CPU可以有效的处理***应用阶段***各种的大规模任务。一般传统上由CPU来处理的任务包括:碰撞检测,全局加速算法(global acceleration algorithms),动画,物理模拟,等等,视具体应用而定。下一个阶段是***几何阶段***,这一阶段要处理移动,投影等。这一阶段计算出什么东西需要被绘制,应该绘制成什么样,在哪里绘制。***几何阶段***一般是在GPU上进行的。GPU包含很多可编程的核心,也有固定处理功能的硬件。最后的***光栅化阶段***利用上个阶段生成的数据进行逐像素的计算生成出最终的图像。***光栅化阶段***是完全在GPU上完成的。这些阶段以及他们内部的管线会在接下来的三个部分里进行讨论。更多关于GPU如何处理这些阶段的内容在第三章给出。
　　
##2.2 应用阶段

　　因为这个阶段是在CPU上执行的,所以我们可以完全控制它。因此,开发者可以决定它的实现并可以在日后修改它以提高性能。在这里进行改变也同样会影响到它后面其它阶段的性能。比如,一个应用阶段的算法或者配置可以减少需要绘制的三角形数量。
　　在应用阶段的最后,需要绘制的几何体会被送到***几何阶段***。它们就是所谓的*渲染图元*(rendering primitives),就像点,线,三角形这些会最终呈现在屏幕(或者任何其它被使用的输出设备)上。这是***应用阶段***最重要的任务。
　　因为这个阶段是基于软件实现的,所以没有办法像***几何阶段***和***光栅化阶段***那样分解成多个子阶段。为了提高性能,这一阶段通常会在多核CPU上并行处理。在CPU设计里,这被称为*超标量*(superscalar)结构,因为在同一时刻上它可以在同一个阶段里执行多个工作。15.5节展示了大量如何利用多核处理器的方法。
　　碰撞检测(collision detection)就是通常在这个阶段实现。当两个物体间的碰撞被检测到。一个响应会被回传给碰撞的物体们以及力反馈设备。这个阶段也是从其它资源里获取输入的地方,像键盘,鼠标,头戴头盔等。根据这些输入,会发生各种不同的行为。其它的一些发生在这个阶段的工作包括,纹理动画,位移动画,或者任何其它阶段没有处理的计算。一些计算算法,比如层次视锥剔除(hierarchical view frustum culling,见14章),也同样在这里实现。

##2.3几何阶段

　　这一阶段负责大部分的逐多边形,逐顶点操作。它被进一步分为如下的功能阶段:模型视图变换,顶点着色,投影,裁剪,屏幕映射(如图2.3)。再次注意,依赖于具体的实现,这些功能阶段会或者不会再管线阶段完全一致。在一些情况下,一组连续的功能阶段可能会组成一个单一的管线阶段(与其他的管线阶段并行执行)。在其它的一些情况下，一个功能阶段会被分成多个更小的管线阶段。
　　![Figure2.3](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.3.jpg)
　　Figure 2.3 几何阶段被分成由一些列功能阶段构成的管线。
　　
　　比如在一个极端情况下,整个渲染管线中的所有阶段都是通过软件实现运行在单一的处理器上的,这时候你可以说你的整个管线只包含一个管线阶段。当然了,这也是在独立的图形加速器出现以前图形处理的方式。在另一种极端情况下,每个功能阶段都会被分成多个更小的管线阶段,且每个管线阶段都会在特定的处理单元上执行。
　　
###2.3.1 模型空间变换

　　一个模型要最终呈现在屏幕上之前会被转换到多个不同的空间或者坐标系统。最初,模型是处于它自己的模型空间的(model pace),也就是说它并不需要任何变换。每个模型都可以被关联一个模型变换矩阵用来进行放置位置和调整朝向。一个单一模型关联多个模型变换矩阵也是可以的。这样就允许一个模型存在多个位置,朝向,缩放不同的拷贝(称为*实例*),而不需要求在原始的几何模型上做任何修改。
　　模型上的顶点以及法线会通过模型矩阵进行变换。模型的坐标被称为模型坐标(model coordinates)。当模型变换被应用在其上之后,就可以说模型已经被转换到*世界坐标系*(world coordinates)或者*世界空间*(world space)了。世界空间是独一无二的,当所有模型都通过他们各自的模型矩阵进行转换以后,他们就都处在同一个空间了。
　　正如我们以前提到过的一样,只有那些可以被相机(或者观察者)看到的模型才会被渲染。相机在世界坐标里有一个位置和朝向,分别用来放置和瞄准。为了便于后续的投影和裁剪。相机以及所有的模型都会进行视图变换(view transform)。视图变换的目的是为了把相机放置到原点,并让他朝向于-Z轴。Y轴指向上方,X轴指向其右侧。进行视图矩阵变换后的实际位置和朝向依赖于所使用的API。这时候所表述的空间被称为相机空间(camera space),或者更一般被称为眼空间(eye space)。一个视图变换如何影响相机和模型的例子如图2.4所示。模型变换和视图变化都是通过4X4的矩阵实现的,这将是第四章讨论的。
　　
　　![Figure 2.4](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.4.jpg)
　　Figure 2.4 在左图中,相机放置按照用户期望的位置和朝向放好。视图转换重新把相机定位到原点,朝着负z轴的方向看,如右图所示。这么做是为了让裁剪和投影操作更简单更快速。图中的淡灰色区域是视见体,这里假定的是透视投影,可以看到视见体是一个截头椎体。类似的技术也被应用到各种类型的投影。
　　
###2.3.2 顶点着色

　　为了创造一个具有真实感的场景,仅仅渲染物体的形状和位置是不够的,它们的外观表现也要进行模拟。这一描述包含了每个物体的材质,以及任何照射在物体上的光源的效果。材质和光源可以通过很多方式进行模拟。从简单的颜色到详尽复杂的物理描述。
　　这个决定光源在材质上的效果的操作就是人们常说的着色(shading)。它会对物体上的每个顶点都进行着色公式计算。一般这种计算都是在***几何阶段***上对模型的顶点进行,也有在***光栅化阶段***计算的。每个顶点都会存储各种材质数据,像位置,法线,颜色,或者任何其它在着色公式里用到数值信息。顶点着色的结果(如颜色,向量,纹理坐标,或任何其它的着色数据)会被送到***光栅化阶段***进行插值。
　　着色计算一般被认为是发生在世界空间的。而实际上有时候把相关计算的实体(如相机和光源)转换到其它的空间(如模型或相机空间)进行计算会更方便。之所以可以这么做是因为只要所有参与着色计算的实体都被转换到同一空间,如光源,相机以及模型,他们的的相对关系是会被保留下来的。
　　着色技术会在整本书中进行深入讨论,特别是在第三章和第五章。
　　
###2.3.3 投影

　　在着色之后,渲染系统要进行投影操作,把视锥转换到一个极值点是(-1,-1,-1)和(1,1,1)的单位立方体中。这个单位立方体被称为*规则视见体*(canonical view volume)。有两种比较常用的投影方式,称为正交投影(也被称为平行)和透视投影,如图2.5所示。
　　![Figure 2.5](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.5.jpg)
　　Figure 2.5 在左边的是正交或者平行投影。右边的是透视投影。
　　
　　正交形式的视见体一般是一个长方体。正交投影把这个视见体转换为一个单位立方体。正交投影的主要特点就是原来平行的线转换完之后依然保持平行。这个转换是平移和缩放的结合。
　　透视投影就有点复杂了,这种投影中,距离相机越远的物体,在投影后呈现的就越小。另外,平行线会交汇于视点。透视转换正是模拟我们人类对于物体的大小感知。从几何上讲,它的视见体被称为截头椎体(frustum),这是一种有着长方形底座的被裁减的椎体 。截头椎体也同样被转换到一个单位立方体中。正交投影和透视投影转换都是可以通过构建4X4的矩阵(见第四章)进行,当它们完成转换,就可以说模型被转换到了*规范化设备坐标系*(normalized device coordinates)了。
　　虽然这些矩阵变换也是从一个空间转换到另一个,但是他们在进行投影之后,z坐标已经不会被生成的图像存储了,所以我们把它们叫做投影。通过这种方式,模型被从三维投影到了二维。
　　
###2.3.4 裁剪
　　
　　只有完全或者部分的在视见体里的图元才需要被传递到***光栅化阶段***,只有它们才会接下来被绘制在屏幕上。那些完全在视见体内部的图元会按照他原本的样子被传递。完全在视见体外部的则不会被传递,因为它们最终并不会被渲染。而那些一部分在视见体内部的图元则需要进行*裁剪*(clipping)。例如一条线段,它的一个顶点在视见体之外,另一个顶点在视见体之内,那么它就会被视见体裁剪。在视见体之外的那个顶点就会被一个位于线段与视见体相交位置的新顶点代替。投影矩阵的使用意味着被转换后的图元是在单位立方体中进行裁剪的。在视图变换和投影之后进行裁剪是比较好的,因为这样可以使裁剪问题具有一致性。所以图元通常都是在单位立方体里进行裁剪的。裁剪的流程如图2.6所示。除了视见体的六个裁剪面以外,用户可以自己定义附加的裁剪面来显式的裁剪物体。在646页的图14.1中展示了一种叫做*切片*(sectioning)的可视化类型。前面提到的几个阶段都是在可编程的处理单元上进行的,而裁剪阶段(以及接下来提到的屏幕映射阶段)通常是通过固定的硬件操作来实现的。
　　
　　![Figure 2.6](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.6.jpg)
　　Figure2.6 经过投影变换之后,只有在单位立方体内的图元(与在视锥体内的图元一致)才需要被继续处理。因此,在单位立方体之外的图元会被丢弃,完全在内部的图元会被保留。如果图元和单位立方体相交,那么它会沿着单位立方体被裁减,新的顶点会创建,旧的被丢弃。
　　
###2.3.5 屏幕映射

　　经过裁剪后依然在视见体内部的图元会被送到***屏幕映射***阶段,刚进入这个阶段的时候图元的坐标还依然三维的。每个图元的x坐标和y坐标将被转换形成屏幕坐标(screen coordinates)。屏幕坐标和z坐标一起被称为窗口坐标(window  coordinates)。假设场景会被渲染到一个最小点在(x1,y1),最大点在(x2,y2)的位置,其中$(x1\lt x2,y1\lt y2)$。***屏幕映射***是平移之后接着一个缩放的操作。新的x,y坐标就可以叫做屏幕坐标了。它们以及z坐标($-1\le z\le 1)$,会被传递到光栅化阶段。屏幕映射的过程如图2.7所示。
　　![Figure 2.7](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.7.jpg)
　　Figure 2.7 在投影变换之后依然在单位立方体内的图元,会经由屏幕映射处理找到它们在屏幕上的坐标。
　　
　　一个经常会让人感到混乱的问题就是整数以及浮点数是怎么关联到像素(和纹理)坐标的。DirectX 9 以及它之前的版本都使用一个把像素中心作为0.0的坐标系统,这意味着[0,9]这段像素覆盖的范围是[-0.5,9.5)。Heckbert[520]提出了一个逻辑上更加一致的方案。给定一列像素并使用笛卡尔坐标系(Cartesian coordinates),最左边像素的左下角作为浮点坐标系的0.0。OpenGL一直使用这个方案,DirectX 10及其后续版本也开始使用。像素的中心位于0.5。所以[0,9]这段像素覆盖的范围是[0.0,10.0）。这个转换很简单
$$ d= floor(c) \tag{2.1} $$ $$c =d+0.5,\tag{2.2}$$
其中$d$是像素的离散序号(整型)而$c$是像素的连续值(浮点型)。
　　在所有的API中像素的位置坐标都是自左向右递增的,而对于上下边界的0位置有时候在OpenGL和 DirectX中是不一致的。OpenGL一向喜欢使用笛卡尔坐标系。把左下角当作最小位置。而DirectX依赖于上下文,有时候会把左上角定位最小位置。它们都有自己的一套逻辑,也不存在谁对谁错。例如,OpenGL(0,0)位于图像的左下角,而DirectX是在左上角。DirectX之所以要这样的原因是,在屏幕上有很多现象都是从上到下的:微软的视窗系统是用这套坐标系统,我们阅读也是这个顺序,而且很多图像文件的格式也是用这种方式存储它们的信息的。要注意的关键是这种差异是存在的,平且当从一个API换到另一个API的时候把这种差异考虑进去很重要。

##2.4 光栅化阶段
　　
　　得到了变换和投影后的顶点以及和顶点相关的着色数据(都是从几何阶段得到),光栅化阶段的目标是计算出一组被物体覆盖的像素的颜色。这个过程被称为光栅化(rasterization)或者扫描转换(scan conversion),也就是把屏幕空间中的二维顶点——每个顶点带有一个Z值(深度值),以及各种与顶点关联的渲染信息转换为屏幕上的像素。
　　![Figure 2.8](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.8.jpg)
　　Figure 2.8 光栅化阶段被分为由一系列功能阶段构成的管线。
　　
　　与***几何阶段***相似,这个阶段也被分成几个功能阶段:三角形装配,三角形便利,像素着色以及合并(如图2.8)。

##2.4.1 三角形装配
　　三角形表面的各种差异及其它的一些数据将在这个阶段计算。这些数据将被用来进行扫描转换，也用来对***几何阶段***生产的那些着色数据进行插值。这个过程是通过特定硬件实现的。

##2.4.2 三角形遍历
　　这个过程是检测每个像素的中心(或者一个采样点)是否被三角形覆盖的地方以及为与三角形重叠的像素生产片元(fragment)的地方。找出哪些采样点或者像素在三角形之内通常被称为*三角形遍历*或者*扫描转换*。三角形片元的的每个属性都是通过把数据在三个三角形顶点内插值得到的(可以查看第五章)。这些属性包含片元的深度,以及任何从***几何阶段***得到的着色数据。Akeley和Jermoluk[7]还有Rogers[1077]提供了更多关于*三角形遍历*的内容。
　　
##2.4.3 像素着色
　　逐像素着色计算在这里进行。利用插值得到的数据作为输入,输出则是一个或者更多要传递到下一阶段的颜色值。与*三角形装配*和*三角形遍历*不同,这些过程都是通过特定的硬件来完成,而像素着色阶段则是通过可编程的GPU核心执行。大量的技术可以在这里使用,其中最重要的一个就是*纹理*。关于纹理的更多细节将在第六章进行讲解。简单说,对一个物体进行纹理操作就是把一个图像贴到它的表面。这个过程如图2.9所示。图像可以是一维,二维,或者三维的,其中二维图像是最常见的。
　　![Figure 2.9](https://github.com/esfog/-Real-Time-Rendering-3rd-Chinese-Translation/raw/master/Images/Chapter2/Figure2.9.jpg)
　　Figure 2.9 左上角展示了一个不带纹理的模型,纹理贴图里的各个区域被"贴"到了龙身上,最后的结果如左下角所示。
　　
##2.4.4 合并
　　每个像素的信息被存储在颜色缓冲区(color buffer),这是一个存储颜色(每个颜色有红绿蓝三个部分)的矩形数组。*合并阶段*主要负责把*着色阶段*产生的片元颜色与当前颜色缓冲区中已有的值进行结合。与*着色阶段*不同,GPU用来处理这个阶段的部分通常是不完全可编程的,不过是高度可配置的,可以产生多种多样的效果。
　　这个阶段也负责解决可见性判断问题。这意味着当整个场景完成渲染后,颜色缓冲区应该只包含那些从相机视角可以看到的图元的颜色信息。对于大多数的图形硬件,都是通过Z缓冲(也叫深度缓冲)算法来实现的。Z缓冲有着和颜色缓冲同样的大小形状。对于每个像素它都会存储一个从相机到当前最近图元的深度。这意味着当一个图元被渲染为一个具体的像素时候,在该图元的这个像素位置上的z值将要同Z缓冲中同一像素位置的值进行比较。如果新的z值比Z缓冲原有值要小,就说明当前要渲染的图元在该像素点位置要比之前已经渲染的图元要距离相机更近。因此,该像素位置的颜色和z值会用正在绘制的图元的颜色和z值更新。如果计算出来的新z值大于Z缓缓冲中的z值那么原来Z缓冲区和颜色缓冲区的值会被保留。Z缓冲算法非常简单,有着$O(n)$的复杂度(这里$n$是将被渲染的图元数量),任何可以逐(关联)像素计算z值的图元绘制都可以使用该算法。同时我们也要注意到这个算法允许大多数片元以任意的顺序渲染,这也是它受欢迎的另一个原因。然而,部分半透明片元不能以任意顺序渲染。他们必须在所有不透明片元渲染完成以后按照从后向前(back-to-front)的顺序渲染(5.7章节)。这也是Z缓冲的一个主要缺点。
　　我们在前面提到过,颜色缓冲区为每个像素保存颜色值,Z缓冲区为每个像素保存z值。不过,还有一些通道和缓冲区用来过滤和捕获片元信息。透明通道(alpha channel)与颜色缓冲区相关联,为每个像素存储着相应的透明度信息(5.7章节)。半透明测试(alpha test)是一个可选的操作,它可以深度测试执行之前对当前片元进行检测。片元的透明度会与一些相关值进行指定类型的比较(相等,大于等于...)。如果片元没有通过这个测试,那么它将被舍弃,不会进行进一步处理。这个测试一般用来保证完全透明的片元不会影响到Z缓冲区(看6.6章节)。
　　*模板缓冲*(stencil buffer)是一个用来记录图元渲染位置的离屏缓冲区。它通常是每个像素包含8位信息。图元可以以各种方式呗渲染到模板缓冲区,缓冲区内的值可以用来控制颜色缓冲区和Z缓冲区的渲染。比如,假设有个绘制有一个实心圆的模板缓冲区。这可以与一个操作符结合使用,对于后续的图元,这个操作符只会允许模板缓冲区实心圆覆盖的位置图元才能被绘制到颜色缓冲区。模板缓冲区是一个强大的工具,可以生成一些特殊效果。以上这些位于管线末端的功能被统称为光栅操作(raster operations)或者混合操作(blend  operations)。
　　在一个系统里帧缓冲区(frame buffer)通常由所有的缓冲区组成,但有时候它纸杯用来表示颜色缓冲区和Z缓冲区的一个组合。1990年,Haeberli和Akeley[474]提出了一个队帧缓冲区的补充,叫作累积缓冲区(accumulation buffer)。在这个缓冲区中,图像可以通过一系列操作来进行累积。比如,一组展示物体运动的图像可以被累积然后再平均一下来产生运动模糊的效果(motion blur)。其它的一些效果也可以通过其实现,如景深(depth of field),抗锯齿(antialiasing),软阴影(soft shadows)等等。
　　当图元们到达并通过了光栅化阶段,那些可以从相机视点看到的部分就会在屏幕上呈现出来。屏幕上显示的是颜色缓冲区的内容。为了避免当图元被光栅化并送到屏幕的过程被人类观察者看到,采用了*双缓冲*(double buffering)技术。这意味场景的渲染是在一个后置缓冲(back buffer)中离屏发生的。一旦场景在后备缓冲中完成了渲染,里面的内容就会与前置缓冲区里面保存的之前在屏幕上显示的内容进行交换。这个交换发生在垂直回扫(vertical retrace)期间,这时候进行这个操作是安全的。
　　更多关于缓冲区和缓冲方法的内容请查看5.6.2节和18.1节。
　　
##2.5 管线回顾

　　一个模型或者物体是由点,线,三角形这些可渲染图元构成的。想像有一个交互式计算机辅助设计(computer aided design(CAD))软件,一位用户正在检查一个手机的设计。这里我们将跟随这个模型穿越整个图形渲染管线,包含三个主要的阶段:应用,几何,光栅化。场景是用正交投影的形式渲染到屏幕窗口的。在这个个简单的例子里,手机模型既包含边线(为了展示边缘部分),又包含三角形()为了展示表面)。一些三角形使用了二维纹理来表示键盘和屏幕。这个例子里,除了纹理应用发生在光栅化阶段以外,其他的着色计算全部在几何阶段完成。
　　
###应用阶段
　　
　　CAD应用程序允许用户选择和移动模型的各个部分。例如,用户会选择电话的顶部然后旋转鼠标来翻转打开手机。应用阶段必须把鼠标的移动转化成相应的旋转矩阵,之后就会看到当渲染时这个矩阵被合适的应用到盖子上。另一个例子是:让相机沿着预先设定好的路线从不同的角度来观察电话的一个动画。相机的参数,如位置,朝向,需要应用程序依据时间来更新。对于需要渲染的每一帧，应用阶段需要为下一个管线中的主要阶段——几何阶段提供相机位置,光照,还有模型的图元。
    
###几何阶段

　　视图变换矩阵以及表示每个物体位置和方向的模型矩阵都是在应用阶段计算出来的。对于每一个传递到几何阶段的物体来说,这两个矩阵通常会乘到一起作为一个单独的矩阵。在几何阶段,顶点和法线会用这个结合后的矩阵来从模型空间转换到视图空间。再利用材质,光源属性来进行顶点着色。　然后进行投影,把物体转换到一个代表着相机所能看到部分的单位立方体空间。所有在立方体以外的图元都会被舍弃。所有和单位立方体相交的图元都会被裁剪来得到一组完全在单位立方体内部的图元。再之后顶点被映射到屏幕上的窗口。在所有这些逐多边形操作结束以后,得到的结果被传递给光栅化阶段——管线中最后一个主要阶段。
　
###光栅化阶段
　　在这个阶段,所有的图元都会被光栅化,被转化成屏幕中的像素。每个物体的每条边没个三角形在屏幕空间瞎进入光栅器,等待被转换。那些与纹理关联的三角形在渲染的时候会把纹理(图像)应用在它们之上。可见性问题是通过Z缓冲算法以及可选的半透明测试和模板测试来解决的。每一个物体都会依次被处理,最后最终的画面被呈现在屏幕上。

##总结

　　这个管线的产生是由于几十年来面向实时渲染应用程序方面的API以及图形硬件的发展而来。有一点很重要,你要知道这个管线结构并不是唯一的一种渲染管线。离线渲染管线经历者另一条不同的发展路程。电影工业方面的渲染常常是利用*微多边形管线*(micropolygon)完成的[196,1236]。学术研究以及如建筑可视化预览这样的预渲染应用程序常常使用*光线追踪*(ray tracing)渲染器(看9.8.2节)。
　　多年来,对于我们在这里所描述的这些流程,应用程序开发者们只能通过所使用的图形API提供的固定功能管线来完成。固定功能管线之所以叫这个名字,就是因为实现它的图形硬件所包含的各个元素并不能以一种灵活的方式来编码。管线的许多部分可以被设置成不同的状态,例如Z缓冲测试(Z-buffer testing)可以被开启或者关闭,但是并不同通过编程来控制不同阶段的功能顺序。一个最新(也许是最后的)使用固定功能管线的机器就是任天堂的Wii。可编程的GPU使精确的决定在整个管线里不同的阶段进行什么操作成为可能。虽然学习固定功能管线提供了一个介绍一些基础原则的合理途径,但大多数新的发展都定位于可编程GPU。这种可编程性也是本书第三版的默认假定,这也是以一种现代方式充分利用GPU。

##扩展阅读及资源

　　Blinn的书《图形管线的一次旅行》(A Trip Down the Graphics Pipeline)[105]是一本关于从投写一个软渲染器的老书,不过这也是一个学习一些实现一个渲染管线的细节的好资源。对于固定功能管线,宝贵的(至今频繁更新)《OpenGL编程宝典》(OpenGL Programming Guide,一般称为红宝书)提供了关于固定功能管线以及使用的相应算法的详细描述。本书的网站, [http://www.realtimerendering.com](http://www.realtimerendering.com),提供了一些关于渲染引擎实现的链接。